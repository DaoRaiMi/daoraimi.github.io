# <center>Redis集群
Redis集群会把数据自动地分布在多个redis实例上。要想使用redis集群，redis必须是3.0及以上的版本的才行。

redis-cluster不支持多个数据库，它默认只有一个库，也就是database 0，所以SELECT命令在集群模式下是不能使用的。

## 集群端口
* 6379
  > 该端口是正常的服务端口，是客户端发起连接的目标端口，和单机的redis的端口是一样的。
* 16379
  > 该端口是在服务端口的基础上加上10000得到的，是一个集群总线(cluster bus)端口，用于节点之间的通讯，采用的是二进制协议。在该端口上执行的功能有：错误检测、配置更新、故障转移授权等等。
  >
  > 该端口可以使用**cluster-port**指令在redis.conf配置文件中修改。

## 数据分片
Redis集群并不使用一致性哈唏来进行数据分片，而是把所有的key都分布在了16384个哈唏槽(hash slots)上，分的方式也比较简单，就是 CRC16(key)%16384

集群中的每个实例都负责维护一段hash slots。比如：
* A节点包含哈唏槽：0 ~ 5500
* B节点包含哈唏槽：5501 ~ 11000
* C节点包含哈唏槽：11001 ~ 16384

这种方式使得在集群中添加或删除节点时变得非常简单。比如：想要添加一个节点D，则只需要从A,B,C三个节点中分一部分hash slots给D即可，同理，如果想删除节点A，则只需要把A节点托管的slots分给B和C，当A节点上的slots为空时，此时可以安全地删除A节点。

由于把slots从一个节点移动到另一个节点并不需要停机操作，所以添加、删除节点，或者调整节点托管的slots的比例，都不需要任何的停机时间。

redis集群也支持多key操作，前提是这些key都属于同一个节点上相同的slot中。因此，用户可以让多个key分布在同一个slot上，就是在创建key的时候，给key添加一个hash tag
> 给key指定hash tag也简单，只需要在key中加一个{}即可，然后把tag写在{}中。比如：`user_{user1}_info` 和 `user_{user1}_token`
>
> 这两个key虽然不同，但是由于{}中的内容是一样的，也就是拥有相同的hash tag，所以这两个key会被分配到相同的slot中。

## 数据一致性
Redis集群并不保证强一致性，也就是说在某些特殊场景下redis-cluster会有数据丢失的风险。这是因为redis cluster使用主从模式来保证每个分片段的数据高可用，并且复制是异步的。比如：
* 客户端向集群中的B发起了写入请求
* B给客户端响应了OK
* B把这个写入操作传递给它的副本实例，B1,B2,和B3

从上面可以看出，B并不等待它的副本实例B1,B2,B3的确认，就向客户端响应了OK。由于主从复制具有延迟，如果当B响应OK之后，还没有来得及把写入操作同步给它的副本节点之前死机了，那么B1,B2,B3中任何一台都没有客户端写入的数据，当它们中的任何一个被提升为master节点后，都会丢失刚刚写入的数据。

其实还有一种情况就是，写入成功并不意味着一定落盘成功，因为并不是实时落盘的。如果采用实时落盘，那么性能会受到影响。

当真正需要的时候，Redis-cluster也支持同步写，是通过**WAIT**命令来实现的。同步写可以做到更少的数据丢失，然而，需要注意的是，redis-cluster并不支持强一致性，即使使用了同步写。

在某些复杂的情况下，未接受到master节点全部命令的slave节点是有可能被提升为master的。比如：当网络发生分区的时候，客户端连接的是具有少数服务器的那个区，并且这个区域内正好有master节点。假设一个集群有6个节点，3主(A,B,C)，3从(A1,B1,C1)，最后还有一个客户端Z1

在网络发生分区时，是有可能出现这种情况的：一边有A,C,A1,B1,C1，另一边有B,Z1。

此时Z1仍然可以向B写入数据，如果分区故障能够在很短的时间内恢复(其实就是在另一边再选一个新的master出来之前),那么整个集群还是能够正常工作的。但是如果分区故障持续的时间足够B1在它所在的分区中被提升为master的话，那么Z1向B中写入的数据就会丢失。

针对这个情况，redis也提供了相应的解决方法，那就是节点超时时间，在超时时间到达后，那么在小数节点内的所有master节点都不会再接受来自客户端的请求了。这样就能够把数据丢失做到最低。

## 配置参数
* cluster-enabled: \<yes/no\>
  > 表示当前节点是否开启redis-cluster功能，默认是no
* cluster-node-timeout: \<milliseconds\>
  > 集群中master节点在被认为是不可用状态时的最大时间。如果超过了该时间，那么该master就会被它的slave节点failover掉。
* cluster-slave-validity-factor: \<factor\>
  > 如果设置为0，则slave节点总是认为自己是有效的，因此它总是尝试去failover，不管slave和master节点之意的连接不可用的时间。
  >
  > 如果设置成正数，则slave节点会等待连接不可用时间超过factor x node-timeout后，才会发起failover。
* cluster-require-full-coverage: \<yes|no\>
* > 集群中是否所有的key都需要被托管，默认是yes.

## 更新集群中的节点
### 更新slave节点
更新slave节点是比较简单的，直接stop，然后再使用新版本的程序启动就行了。客户端那边他们会自动的重连的。
### 更新主节点
更新master节点要稍微麻烦一点，步骤如下：

* 在某一个slave节点(该节点会变成master)上执行**cluster failover takeover**命令
* 等待对应分片中的master变成slave节点
* 按照更新slave节点的方式来更新旧的master节点。
* 在更新完成后，如果还想让当前这个旧的master重新变成分片的master的话，那就么在旧的master上再次执行**cluster failover takeover**命令即可。